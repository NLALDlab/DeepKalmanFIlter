{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Kalman Filter example with a 3-dof mass-spring-damper model: linear model, known with parameter mismatch, measured variables:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "DKFDirectory = '..'\n",
    "sys.path.append(DKFDirectory)\n",
    "import CreateTrainingSet_3_masses_4DKF\n",
    "\n",
    "from DeepKalmanFilter.main import main\n",
    "from DeepKalmanFilter.ConstructLaplacianMatrices import *\n",
    "from DeepKalmanFilter.ConstructSGMatrices import *\n",
    "from DeepKalmanFilter.ConstructTVMatrices import *\n",
    "from DeepKalmanFilter.Utility import *\n",
    "WorkingDirectory = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation:\n",
    "CreateTrainingSet_3_masses_4DKF.create(Experiment='3MKC1',M1=2.7,K1=1e4,C1=1e1,epsilon=1.e0,load_0=1.e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingBatchNum = 300 #150 #1000\n",
    "TrainingBatchSize = 1\n",
    "\n",
    "# DKF parameters setting:\n",
    "NetParameters = {}\n",
    "### General parameters:\n",
    "NetParameters['Experiment'] = sio.loadmat('Experiment.mat',squeeze_me = True)['Experiment']\n",
    "NetParameters['Layers'] = sio.loadmat(f'LayersExp{NetParameters[\"Experiment\"]}.mat',squeeze_me = True)['Layers']\n",
    "NetParameters['Model'] = loadPickle(f'ModelExp{NetParameters[\"Experiment\"]}.mat')\n",
    "NetParameters['C'] = sio.loadmat(f'CExp{NetParameters[\"Experiment\"]}.mat',squeeze_me = False)['C']\n",
    "NetParameters['StateDimension'] = NetParameters['C'].shape[1]\n",
    "NetParameters['ObservationDimension'] = NetParameters['C'].shape[0]\n",
    "NetParameters['WeightMats'] = 'Input'  #Supported values: Input, Identity  \n",
    "NetParameters['HiddenDynamicsNumber'] = 1\n",
    "NetParameters['HiddenDynamicsDimension'] = [sio.loadmat(f'HiddenDynDimExp{NetParameters[\"Experiment\"]}.mat',squeeze_me = True)['HiddenDynDim']]*NetParameters['HiddenDynamicsNumber']\n",
    "NetParameters['ActivateModelDiscovery'] = 'No' #'No','Yes'\n",
    "### Learning parameters:\n",
    "NetParameters['SharedWeights'] = 'No' #'No','Yes'\n",
    "NetParameters['BackPropagation'] = 'Truncated' #'Complete','Truncated'                                           #Supported values: Complete, Truncated  \n",
    "NetParameters['ProjectDynamics'] = 'No' #'No','Yes'\n",
    "NetParameters['Jacobians'] = 'Approximated' #'Approximated', 'Algebraic'  \n",
    "NetParameters['FiniteDifferences'] = 'Central' #'Supported values: 'Forward', 'Backward', 'Central'  \n",
    "NetParameters['FiniteDifferencesSkip'] = 1.e-2 \n",
    "NetParameters['GainLearningRate'] = (1e-2) / TrainingBatchSize\n",
    "NetParameters['GainLearningRateReduction'] = 0.8 #1\n",
    "NetParameters['GainLearningRateIncrease'] = 1e2\n",
    "NetParameters['DynamicsLearningRate'] = (1.e0) / TrainingBatchSize\n",
    "NetParameters['DynamicsLearningRateReduction'] = 0.2\n",
    "### Loss function parameters:\n",
    "Pen1Val = 1e0\n",
    "Pen2Val = np.ones(NetParameters['Layers']) * 1e0\n",
    "NormPen = max(Pen1Val, Pen2Val.max())\n",
    "NetParameters['Penalty0'] = 1.e6\n",
    "NetParameters['Penalty1'] = np.ones(NetParameters['Layers']) * Pen1Val / NormPen\n",
    "NetParameters['Penalty2'] = Pen2Val / NormPen\n",
    "NetParameters['Penalty3'] = 1.e0 / (NetParameters['StateDimension'] * NetParameters['ObservationDimension'])\n",
    "NetParameters['Penalty4'] = 0.01 #0.2 # soft-thresholding\n",
    "#NetParameters['WinLen'] = 31\n",
    "#NetParameters['StencilA0'], NetParameters['StencilA1'] = ConstructSGMatrices(NetParameters['WinLen'])\n",
    "NetParameters['L'], NetParameters['LtL'] = ConstructLaplacianMatrices( NetParameters['Layers'], NetParameters['Model']['SamplingTimes'] )\n",
    "### Adam (learning optimizer) parameters:\n",
    "NetParameters['Optimizer'] = 'Adam'\n",
    "NetParameters['BetaMoment1'] = 0.9\n",
    "NetParameters['BetaMoment2'] = 0.999\n",
    "NetParameters['Initialization'] = 'Deterministic'   #'Random', 'Deterministic', 'DeterministcComplete'  \n",
    "NetParameters['InitializationMean'] = 0\n",
    "NetParameters['InitializationSigma'] = 0.0001\n",
    "NetParameters['AdamEpsilon'] = 1e-16\n",
    "### Learning stop condition parameters:\n",
    "NetParameters['TrainingConditionStop'] = 'Residues' #'Whiteness', 'Residues'  \n",
    "NetParameters['ResidueDecreaseThreshold'] = 1e-3\n",
    "NetParameters['ActivateWhitenessMask'] = 'Yes'\n",
    "NetParameters['WhitenessLagCounter'] = 1\n",
    "NetParameters['WhitenessIterationCheck'] = 20\n",
    "NetParameters['WhitenessUpdateCheck'] = 8\n",
    "NetParameters['WhitenessDecreaseThreshold'] = -1e-3\n",
    "### Model discovery parameters:\n",
    "NetParameters['DictionaryBlocks'] = ['Constant', 'Linear', 'Quadratic', 'Cubic']\n",
    "NetParameters['AllowedDictionaryBlocks'] = {\n",
    "    'Constant': 1,\n",
    "    'Linear': NetParameters['StateDimension'],\n",
    "    'Quadratic': NetParameters['StateDimension'] * (NetParameters['StateDimension'] + 1) // 2,\n",
    "    'Cubic': NetParameters['StateDimension'] * (NetParameters['StateDimension'] + 1) * (NetParameters['StateDimension'] + 2) // 6\n",
    "}\n",
    "NetParameters['DictionaryDimension'] = sum(NetParameters['AllowedDictionaryBlocks'][block] for block in NetParameters['DictionaryBlocks'])\n",
    "NetParameters['ModelDiscoveryForceCheck'] = 1000\n",
    "NetParameters['ModelDiscoveryUpdateBoth'] = 'Yes'\n",
    "NetParameters['ModelDiscoveryMethod'] = 'OMP'                                            #Supported values: OMP, LH  \n",
    "NetParameters['ModelDiscoverySmoothing'] = 'SGMixed2'                                    #Supported values: TV, TVMixed, SG, SGMixed1, SGMixed2\n",
    "NetParameters['ModelDiscoveryFirstState'] = min(0, NetParameters['Layers'] // 2)\n",
    "NetParameters['A'], NetParameters['D'], NetParameters['AtA'], NetParameters['B'] = ConstructTVMatrices( NetParameters['Layers'] - NetParameters['ModelDiscoveryFirstState'], NetParameters['Model']['SamplingTimes'] )\n",
    "NetParameters['WinLen'] = 31\n",
    "NetParameters['StencilA0'], NetParameters['StencilA1'] = ConstructSGMatrices(NetParameters['WinLen'])\n",
    "NetParameters['ModelDiscoveryRelativeThreshold'] = 0.8\n",
    "NetParameters['ModelDiscoveryStblSuppCondition'] = 4\n",
    "NetParameters['ModelDiscoveryStblSuppUpdates'] = 1\n",
    "NetParameters['OMPSparsity'] = 1\n",
    "\n",
    "# Save net parameters\n",
    "sio.savemat('DefaultNetParameters.mat', {'NetParameters': NetParameters})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main(WorkingDirectory,NetParameters,TrainingBatchSize,TrainingBatchNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = None\n",
    "print(None*M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.fft import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Training:\n",
    "#\n",
    "# initialize network weights:\n",
    "NetWeights = InitializeWeights(NetParameters)\n",
    "# Load the training set\n",
    "WorkingTrainingSetName = f'LatestTrainingSetExp{NetParameters[\"Experiment\"]}.mat'\n",
    "TrainingSet = loadPickle(WorkingTrainingSetName)\n",
    "# Setup dimensions\n",
    "TrainInstancesNum = np.shape(TrainingSet[0])[0]\n",
    "TrainingResidues = np.zeros((TrainingBatchNum,1))\n",
    "OverallStateEstimationError = np.zeros((TrainingBatchNum,1))\n",
    "PeriodogramResidues = np.zeros((TrainingBatchNum, 2 * NetParameters['ObservationDimension'], TrainInstancesNum))\n",
    "# Set up the figure\n",
    "plt.figure(1)\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "# Initialize the moments\n",
    "Moment1, Moment2 = InitializeGradsAndMoments(NetWeights, NetParameters)[1:]\n",
    "AdamInd = 1\n",
    "# Compute weight matrices\n",
    "MeasurementWeightMats, PredictorWeightMats, MeasurementWeightMatsSym, PredictorWeightMatsSym = ComputeWeightMats(NetParameters)\n",
    "\n",
    "# Cycle over batch number\n",
    "for TrainingBatchInd in range(1, TrainingBatchNum+1):\n",
    "    # Reset gradients for new batch but keep the moments intact\n",
    "    Grads = InitializeGradsAndMoments(NetWeights, NetParameters)[0]\n",
    "\n",
    "    # Cycle over each training instance in the batch\n",
    "    for BatchInd in range(1, TrainingBatchSize+1):\n",
    "        # Randomly select a training instance\n",
    "        TrainInstanceInd = BatchInd-1\n",
    "\n",
    "        # Extract instance\n",
    "        Inputs = TrainingSet[0][TrainInstanceInd]\n",
    "        Measurements = TrainingSet[1][TrainInstanceInd]\n",
    "        FirstState = TrainingSet[2][TrainInstanceInd]\n",
    "        TrajectoryTrue = TrainingSet[3][TrainInstanceInd]\n",
    "        StateTrue = TrainingSet[4][TrainInstanceInd]\n",
    "        Dynamic = TrainingSet[5][TrainInstanceInd]\n",
    "        \n",
    "        # Print progress\n",
    "        #OverallProgress = f'Training batch number: {TrainingBatchInd}/{TrainingBatchNum}. \\nCurrently processing batch instance: {BatchInd}/{TrainingBatchSize}. \\n'\n",
    "        #print(OverallProgress, end='')\n",
    "\n",
    "        #PropagationProgress = 'Propagating instance...\\n'\n",
    "        #print(PropagationProgress, end='')\n",
    "\n",
    "        # Propagate input\n",
    "        States, MeasurementMinusCStates, GainMeasurementMinusCFs, MeasurementMinusCFs, FStateDynInputs = PropagateInput(Inputs, Measurements, FirstState, Dynamic, F, NetWeights, NetParameters, decode)\n",
    "        #if TrainingBatchInd > 0: plt.pause(30.0)\n",
    "        # Assemble gains tensor\n",
    "        TensorizedGains = ConstructTensorizedGains(NetWeights, NetParameters)\n",
    "\n",
    "        # Update training residue, cumulative periodograms residues and assemble states evolution\n",
    "        ShowStates = np.zeros((NetParameters['Layers']+1, NetParameters['StateDimension']))\n",
    "        ShowCorrectorResidues = np.zeros((NetParameters['Layers'], NetParameters['ObservationDimension']))\n",
    "        ShowPredictorResidues = np.zeros((NetParameters['Layers'], NetParameters['ObservationDimension']))\n",
    "        ShowMeasurements = np.zeros((NetParameters['Layers']+1, NetParameters['ObservationDimension']))  # TO BE REMOVED\n",
    "        ShowStates[0:1,:] = FirstState.T\n",
    "        ShowMeasurements[0:1,:] = Measurements[:,0:1].T\n",
    "\n",
    "        for Layer in range(1, NetParameters['Layers'] + 1):\n",
    "            TrainingResidues[TrainingBatchInd-1,0] += np.squeeze( (NetParameters['Penalty1'][Layer-1]/2)*( MeasurementMinusCStates[:,[Layer-1]].T )@MeasurementWeightMats[Layer-1]@( MeasurementMinusCStates[:,[Layer-1]] )/TrainingBatchSize + (NetParameters['Penalty2'][Layer-1]/2)*( GainMeasurementMinusCFs[:,[Layer-1]].T )@PredictorWeightMats[Layer-1]@( GainMeasurementMinusCFs[:,[Layer-1]] )/TrainingBatchSize )\n",
    "            ShowStates[Layer,:] = States[:,[Layer]].T\n",
    "            ShowMeasurements[Layer,:] = Measurements[:,Layer:Layer+1].T  # TO BE REMOVED\n",
    "            ShowCorrectorResidues[Layer-1,:] = MeasurementMinusCStates[:,[Layer-1]].T\n",
    "            ShowPredictorResidues[Layer-1,:] = GainMeasurementMinusCFs[:,[Layer-1]].T\n",
    "        #endfor\n",
    "        TrainingResidues[TrainingBatchInd-1] += np.squeeze( (NetParameters['Penalty0']/2)*np.linalg.norm( States[:,-1] - StateTrue )**2/TrainingBatchSize + (NetParameters['Penalty3']/2)*np.linalg.norm( np.tensordot(TensorizedGains, NetParameters['L'], axes=([2], [1])) )**2/TrainingBatchSize )\n",
    "        PeriodogramResidues[TrainingBatchInd-1,:,TrainInstanceInd-1] += ComputePeriodogramResidue(MeasurementMinusCStates, MeasurementMinusCFs)\n",
    "        OverallStateEstimationError[TrainingBatchInd-1] = np.linalg.norm(np.squeeze(TrainingSet[3][0][:,:]) - np.squeeze(States[:,:]),'fro')\n",
    "        # Check whiteness\n",
    "        #if ( (NetParameters['ActivateWhitenessMask'] == 'Yes') and (not InhibitWhitenessCheck) and (TrainingBatchInd > NetParameters['WhitenessIterationCheck'] and AdamInd > NetParameters['WhitenessUpdateCheck']) ):\n",
    "        #    StopCond = (PeriodogramResidues[TrainingBatchInd-1,NetParameters['ObservationDimension']:,TrainInstanceInd-1] - PeriodogramResidues[TrainingBatchInd-2,NetParameters['ObservationDimension']:,TrainInstanceInd-1] < NetParameters['WhitenessDecreaseThreshold']).T\n",
    "        ##endif\n",
    "\n",
    "        #BackPropagationProgress = 'Back-propagating instance...\\n'\n",
    "        #print(BackPropagationProgress, end='')\n",
    "\n",
    "        # Compute jacobians\n",
    "        StateJacobians, DynJacobians = ComputeJacobians(F, States, None, Inputs, None, None, FStateDynInputs, NetParameters)\n",
    "\n",
    "        # Backpropagate output\n",
    "        Grads = BackPropagateOutput(StateTrue, Dynamic, States, MeasurementMinusCStates, GainMeasurementMinusCFs, MeasurementMinusCFs, FStateDynInputs, TensorizedGains, MeasurementWeightMatsSym, PredictorWeightMatsSym, Grads, StateJacobians, DynJacobians, NetWeights, NetParameters)\n",
    "    #endfor\n",
    "\n",
    "    #UpdateWeightsProgress = 'Updating weights...\\n'\n",
    "    #print(UpdateWeightsProgress, end='')\n",
    "\n",
    "    # Update net weights\n",
    "    #ERA COSì: NetWeights, Moment1, Moment2 = UpdateWeights(NetWeights, copy.deepcopy(Grads), Moment1, Moment2, Dynamic, AdamInd, np.ones((NetParameters['StateDimension'], 1)) - NetParameters['C'].T@( np.logical_not(np.sum(LaggedGainMask, axis=1))*1 ), NetParameters)\n",
    "    NetWeights, Moment1, Moment2 = UpdateWeights(NetWeights, copy.deepcopy(Grads), Moment1, Moment2, Dynamic, AdamInd, np.ones((NetParameters['StateDimension'], 1)), NetParameters)\n",
    "\n",
    "    AdamInd += 1\n",
    "\n",
    "    #print('\\b' * len(UpdateWeightsProgress), end='')\n",
    "\n",
    "    # Adaptively change the learning rates\n",
    "    if TrainingResidues[TrainingBatchInd-1] < NetParameters['GainLearningRate']:\n",
    "        NetParameters['GainLearningRate'] *= NetParameters['GainLearningRateReduction']\n",
    "    #endif\n",
    "    #if ( (NetParameters['TrainingConditionStop'] == 'Residues') and InhibitWhitenessCheck and (AdamInd > NetParameters['WhitenessUpdateCheck']) and ( (TrainingResidues[TrainingBatchInd-2] - TrainingResidues[TrainingBatchInd-1] < NetParameters['ResidueDecreaseThreshold']) and (TrainingResidues[TrainingBatchInd-2] - TrainingResidues[TrainingBatchInd-1] > 0) ) ):\n",
    "    #    # Early stop training condition based on residues decrease\n",
    "    #    StopTraining = 1\n",
    "\n",
    "    # Show training output\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(1, figsize=(20, 10))\n",
    "    plt.clf()\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    #plt.plot(StateTrue, 'b+-')\n",
    "    #plt.plot(States[-1], 'm.-')\n",
    "    #plt.title('Output state comparison')\n",
    "    #plt.xlabel('Nodes')\n",
    "    #plt.legend(['True','Estimated'],loc='upper right')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(np.diag(NetWeights[-2]),'b-')\n",
    "    plt.title('np.diag(NetWeights[-2])')\n",
    "    \n",
    "    ix = 2\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(np.squeeze(TrainingSet[3][0][:,[-1]]),'b-')\n",
    "    #plt.plot(np.squeeze(Xpred_hist[[ix],:]),'c--')\n",
    "    plt.plot(np.squeeze(States[:,[-1]].T),'m--')\n",
    "    plt.title('state estimation at last layer')\n",
    "    plt.grid()\n",
    "    #plt.subplot(3, 3, 3)\n",
    "    #plt.imshow(NetWeights[-1][-1], aspect='auto', cmap='viridis')\n",
    "    #plt.colorbar()\n",
    "    #plt.title('Reconstructed unmodeled dynamics')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.semilogy(range(1, TrainingBatchInd + 1), OverallStateEstimationError[:TrainingBatchInd],'b-')\n",
    "    plt.title('Overall state estimation error at each iterate')\n",
    "    plt.xlabel('Iterate')\n",
    "    plt.ylabel('Overall state estimation error')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    for ix in range(1,States.shape[1]):\n",
    "        plt.semilogy(ix,np.linalg.norm(np.squeeze(TrainingSet[3][0][:,[ix]]) - np.squeeze(States[:,[ix]].T)),'b.')\n",
    "    #endfor\n",
    "    plt.title('Current state estimation error at each layer')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('state estimation error')\n",
    "    #plt.gca().set_prop_cycle(color=plt.cm.hsv(np.linspace(0, 1, NetParameters['ObservationDimension']+1)))\n",
    "    #plt.semilogy(range(1, TrainingBatchInd + 1), PeriodogramResidues[:TrainingBatchInd, :NetParameters['ObservationDimension'],0])\n",
    "    #plt.legend([f'ObservedState:{i+1}' for i in range(NetParameters['ObservationDimension'])], loc='lower left')\n",
    "    #plt.title('Running corrector residue periodogram (average over batch)')\n",
    "    #plt.xlabel('Iterate')\n",
    "    #plt.ylabel('Periodogram residues (states)')\n",
    "\n",
    "    #ix = 20\n",
    "    plt.subplot(2, 3, 6)\n",
    "    #plt.plot(np.squeeze(TrainingSet[3][0][:,[ix]]),'b-')\n",
    "    ##plt.plot(np.squeeze(Xpred_hist[[ix],:]),'c--')\n",
    "    #plt.plot(np.squeeze(States[:,[ix]].T),'m--')\n",
    "    #plt.title('state estimation at layer '+str(ix))\n",
    "    #plt.grid()\n",
    "    #plt.gca().set_prop_cycle(color=plt.cm.hsv(np.linspace(0, 1, NetParameters['ObservationDimension']+1)))\n",
    "    #plt.semilogy(PeriodogramResidues[:TrainingBatchInd, NetParameters['ObservationDimension']:,0])\n",
    "    #plt.legend([f'ObservedState:{i+1}' for i in range(NetParameters['ObservationDimension'])], loc='lower left')\n",
    "    #plt.title('Running predictor residue periodogram (average over batch)')\n",
    "    #plt.xlabel('Iterate')\n",
    "    #plt.ylabel('Periodogram residues (states)')\n",
    "\n",
    "    #plt.subplot(3, 3, 7)\n",
    "    #plt.gca().set_prop_cycle(color=plt.cm.hsv(np.linspace(0, 1, 2 * NetParameters['StateDimension']+1)))\n",
    "    #plt.plot(np.hstack((ShowStates @ NetParameters['C'].T, ShowMeasurements)))\n",
    "    #plt.legend([f'EstimatedState:{i+1}' for i in range(NetParameters['ObservationDimension'])] * 2, loc='upper left')\n",
    "    #plt.title('States estimates')\n",
    "    #plt.xlabel('Nodes')\n",
    "\n",
    "    #plt.subplot(3, 3, 8)\n",
    "    #plt.gca().set_prop_cycle(color=plt.cm.hsv(np.linspace(0, 1, NetParameters['StateDimension']+1)))\n",
    "    #plt.semilogy(np.abs(ShowCorrectorResidues))\n",
    "    #plt.legend([f'CorrectorResidues:{i+1}' for i in range(NetParameters['ObservationDimension'])], loc='upper left')\n",
    "    #plt.title('Corrector Residues')\n",
    "    #plt.xlabel('Nodes')\n",
    "\n",
    "    #plt.subplot(3, 3, 9)\n",
    "    #plt.gca().set_prop_cycle(color=plt.cm.hsv(np.linspace(0, 1, NetParameters['StateDimension']+1)))\n",
    "    #plt.semilogy(np.abs(ShowPredictorResidues))\n",
    "    #plt.legend([f'PredictorResidues:{i+1}' for i in range(NetParameters['ObservationDimension'])], loc='upper left')\n",
    "    #plt.title('Predictor Residues')\n",
    "    #plt.xlabel('Nodes')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.draw(); plt.show()\n",
    "    plt.pause(0.1)\n",
    "#endfor\n",
    "print('********************************************************************************')\n",
    "print('Training completed.')\n",
    "print('Updated weights for the net have been saved and are ready to be used.')\n",
    "print('********************************************************************************\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(NetParameters['Penalty4'])\n",
    "print(OverallStateEstimationError[-1])\n",
    "print(np.linalg.norm(np.squeeze(TrainingSet[3][0][:,[-1]]) - np.squeeze(States[:,[-1]].T)))\n",
    "print(len(np.where(diagC > 0.0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_s = [0.0,1e-09,0.01,0.1,0.2]\n",
    "n2_overall = [173,503,503,844]\n",
    "n2_final = [8.1,48,48,76]\n",
    "d = [51,5,5,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.info(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = np.array([1.0, None, 3.0])\n",
    "print((v == 2).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "M = NetWeights[k-1] - L_hist[:,:,k]\n",
    "[U,S,V] = np.linalg.svd(M); V = V.T\n",
    "plt.figure()\n",
    "plt.semilogy(np.diag(np.sort(S)[-1::-1]),'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array(MeasurementMinusCStates[:,k-1],dtype=np.float64)\n",
    "cm1 = np.corrcoef(np.atleast_2d(V[:,-1]),v1); print(\"corr(e,V[:,-1]) = \",cm1[1,0])\n",
    "cm2 = np.corrcoef(np.atleast_2d(V[:,-2]),v1); print(\"corr(e,V[:,-2]) = \",cm2[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array([1,2,3]); print(\"v1.shape = \",v1.shape)\n",
    "v2 = np.array([1,2,3]); print(\"v2.shape = \",v2.shape)\n",
    "print(np.corrcoef(v1,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array(MeasurementMinusCStates[:,k-1],dtype=np.float64)\n",
    "print(type(v1[0]))\n",
    "print(\"v1.shape = \",v1.shape)\n",
    "v2 = V[:,-1]\n",
    "print(type(v2[0]))\n",
    "print(\"v2.shape = \",v2.shape)\n",
    "np.corrcoef(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(np.corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.diag(NetWeights[-2]),'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_noCtraining = TrainingResidues[:TrainingBatchInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(res_noCtraining,'b-')\n",
    "plt.plot(TrainingResidues[:TrainingBatchInd],'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
